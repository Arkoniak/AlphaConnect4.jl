{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "iteration               1\n",
      "loss on plotted curve   0.432\n",
      "-----------------------------\n",
      "iteration               1000\n",
      "loss on plotted curve   0.317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ee44fba83e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mmbinds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mntrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmbinds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmbinds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Interpolate between current weights and trained weights from this task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# I.e. (weights_before - weights_after) is the meta-gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, autograd as ag\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "seed = 0\n",
    "plot = True\n",
    "innerstepsize = 0.02 # stepsize in inner SGD\n",
    "innerepochs = 1 # number of epochs of each inner SGD\n",
    "outerstepsize0 = 0.1 # stepsize of outer optimization, i.e., meta-optimization\n",
    "niterations = 30000 # number of outer updates; each iteration we sample one task and update on it\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Define task distribution\n",
    "x_all = np.linspace(-5, 5, 50)[:,None] # All of the x points\n",
    "ntrain = 10 # Size of training minibatches\n",
    "def gen_task():\n",
    "    \"Generate classification problem\"\n",
    "    phase = rng.uniform(low=0, high=2*np.pi)\n",
    "    ampl = rng.uniform(0.1, 5)\n",
    "    f_randomsine = lambda x : np.sin(x + phase) * ampl\n",
    "    return f_randomsine\n",
    "\n",
    "def gen_task2(phase, ampl):\n",
    "    f_randomsine = lambda x : np.sin(x + phase) * ampl\n",
    "    return f_randomsine\n",
    "\n",
    "# Define model. Reptile paper uses ReLU, but Tanh gives slightly better results\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "def totorch(x):\n",
    "    return ag.Variable(torch.Tensor(x))\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = totorch(x)\n",
    "    y = totorch(y)\n",
    "    model.zero_grad()\n",
    "    ypred = model(x)\n",
    "    loss = (ypred - y).pow(2).mean()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data -= innerstepsize * param.grad.data\n",
    "\n",
    "def predict(x):\n",
    "    x = totorch(x)\n",
    "    return model(x).data.numpy()\n",
    "\n",
    "# Choose a fixed task and minibatch for visualization\n",
    "\n",
    "f_plot = gen_task()\n",
    "with open(\"eval_task.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        phase, ampl = [float(x) for x in line.split(\",\")]\n",
    "        f_plot = gen_task2(phase, ampl)\n",
    "\n",
    "xtrain_plot = x_all[rng.choice(len(x_all), size=ntrain)]\n",
    "with open(\"eval_ids.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        xtrain_plot = x_all[[int(x) for x in line.split(\",\")]]\n",
    "\n",
    "tasks = []\n",
    "with open(\"tasks.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        tasks.append([float(x) for x in line.split(\",\")])\n",
    "\n",
    "ext_inds = []\n",
    "with open(\"ext_inds.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        ext_inds.append([int(x) for x in line.split(\",\")])\n",
    "        \n",
    "# Reptile training loop\n",
    "for iteration in range(niterations):\n",
    "    weights_before = deepcopy(model.state_dict())\n",
    "    # Generate task\n",
    "    f = gen_task2(tasks[iteration][0], tasks[iteration][1])\n",
    "    y_all = f(x_all)\n",
    "    # Do SGD on this task\n",
    "    # inds = rng.permutation(len(x_all))\n",
    "    inds = ext_inds[iteration]\n",
    "    for _ in range(innerepochs):\n",
    "        for start in range(0, len(x_all), ntrain):\n",
    "            mbinds = inds[start:start+ntrain]\n",
    "            train_on_batch(x_all[mbinds], y_all[mbinds])\n",
    "    # Interpolate between current weights and trained weights from this task\n",
    "    # I.e. (weights_before - weights_after) is the meta-gradient\n",
    "    weights_after = model.state_dict()\n",
    "    outerstepsize = outerstepsize0 * (1 - iteration / niterations) # linear schedule\n",
    "    model.load_state_dict({name : \n",
    "        weights_before[name] + (weights_after[name] - weights_before[name]) * outerstepsize \n",
    "        for name in weights_before})\n",
    "\n",
    "    # Periodically plot the results on a particular task and minibatch\n",
    "    if plot and iteration==0 or (iteration+1) % 1000 == 0:\n",
    "        plt.cla()\n",
    "        f = f_plot\n",
    "        weights_before = deepcopy(model.state_dict()) # save snapshot before evaluation\n",
    "        plt.plot(x_all, predict(x_all), label=\"pred after 0\", color=(0,0,1))\n",
    "        for inneriter in range(32):\n",
    "            train_on_batch(xtrain_plot, f(xtrain_plot))\n",
    "            if (inneriter+1) % 8 == 0:\n",
    "                frac = (inneriter+1) / 32\n",
    "                plt.plot(x_all, predict(x_all), label=\"pred after %i\"%(inneriter+1), color=(frac, 0, 1-frac))\n",
    "        plt.plot(x_all, f(x_all), label=\"true\", color=(0,1,0))\n",
    "        lossval = np.square(predict(x_all) - f(x_all)).mean()\n",
    "        plt.plot(xtrain_plot, f(xtrain_plot), \"x\", label=\"train\", color=\"k\")\n",
    "        plt.ylim(-4,4)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.pause(0.01)\n",
    "        model.load_state_dict(weights_before) # restore from snapshot\n",
    "        print(\"-----------------------------\")\n",
    "        print(\"iteration               {}\".format(iteration+1))\n",
    "        print(\"loss on plotted curve   {:.3f}\".format(lossval)) # would be better to average loss over a set of examples, but this is optimized for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.8851604956705836, 1.634926426631627]\n"
     ]
    }
   ],
   "source": [
    "with open(\"eval_task.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        print([float(x) for x in line.split(\",\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.91836735],\n",
       "       [ 4.79591837],\n",
       "       [-2.14285714],\n",
       "       [-1.12244898],\n",
       "       [-5.        ],\n",
       "       [ 4.18367347],\n",
       "       [-3.36734694],\n",
       "       [ 1.53061224],\n",
       "       [ 0.51020408],\n",
       "       [ 3.16326531]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"eval_ids.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        xtrain_plot = x_all[[int(x) for x in line.split(\",\")]]\n",
    "\n",
    "xtrain_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tasks = []\n",
    "with open(\"tasks.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        tasks.append([float(x) for x in line.split(\",\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0221422 ],\n",
       "       [ 0.55535251],\n",
       "       [-0.85915786],\n",
       "       [-0.84218323],\n",
       "       [-0.37831536],\n",
       "       [ 0.32682943],\n",
       "       [ 0.05167898],\n",
       "       [ 0.80155498],\n",
       "       [-0.12587641],\n",
       "       [ 0.26184642],\n",
       "       [-0.30406606],\n",
       "       [-0.12208395],\n",
       "       [-1.06078362],\n",
       "       [-0.74192226],\n",
       "       [-0.48071307],\n",
       "       [ 0.06067892],\n",
       "       [ 0.59191287],\n",
       "       [ 0.69817364],\n",
       "       [-0.72751081],\n",
       "       [-0.58355033],\n",
       "       [ 0.25810301],\n",
       "       [ 0.92219269],\n",
       "       [-0.21943124],\n",
       "       [ 0.85931396],\n",
       "       [-0.10027938],\n",
       "       [ 0.08487212],\n",
       "       [ 0.97331411],\n",
       "       [-0.95175725],\n",
       "       [-0.71544433],\n",
       "       [-0.23541121],\n",
       "       [-0.42948088],\n",
       "       [ 0.89139748],\n",
       "       [-0.76156646],\n",
       "       [-0.47726426],\n",
       "       [-0.7944538 ],\n",
       "       [-0.97052705],\n",
       "       [-0.71252966],\n",
       "       [ 0.95814681],\n",
       "       [ 0.50040412],\n",
       "       [ 0.41395468],\n",
       "       [ 0.01961884],\n",
       "       [-0.36207476],\n",
       "       [ 0.0747766 ],\n",
       "       [-0.97882289],\n",
       "       [-0.79214132],\n",
       "       [-0.59230739],\n",
       "       [ 0.73408157],\n",
       "       [ 0.75951695],\n",
       "       [-0.54085475],\n",
       "       [-0.03548805],\n",
       "       [ 0.70868784],\n",
       "       [ 1.01554716],\n",
       "       [ 0.3598974 ],\n",
       "       [ 0.18038218],\n",
       "       [ 0.70964849],\n",
       "       [-0.60463977],\n",
       "       [ 0.16329235],\n",
       "       [-0.83828813],\n",
       "       [-0.71010029],\n",
       "       [-0.52556252],\n",
       "       [ 0.6258198 ],\n",
       "       [ 0.40616217],\n",
       "       [-0.6864723 ],\n",
       "       [ 0.35487333]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['0.weight'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0221422 ]\n",
      " [ 0.55535251]\n",
      " [-0.85915786]\n",
      " [-0.84218323]\n",
      " [-0.37831536]\n",
      " [ 0.32682943]\n",
      " [ 0.05167898]\n",
      " [ 0.80155498]\n",
      " [-0.12587641]\n",
      " [ 0.26184642]\n",
      " [-0.30406606]\n",
      " [-0.12208395]\n",
      " [-1.06078362]\n",
      " [-0.74192226]\n",
      " [-0.48071307]\n",
      " [ 0.06067892]\n",
      " [ 0.59191287]\n",
      " [ 0.69817364]\n",
      " [-0.72751081]\n",
      " [-0.58355033]\n",
      " [ 0.25810301]\n",
      " [ 0.92219269]\n",
      " [-0.21943124]\n",
      " [ 0.85931396]\n",
      " [-0.10027938]\n",
      " [ 0.08487212]\n",
      " [ 0.97331411]\n",
      " [-0.95175725]\n",
      " [-0.71544433]\n",
      " [-0.23541121]\n",
      " [-0.42948088]\n",
      " [ 0.89139748]\n",
      " [-0.76156646]\n",
      " [-0.47726426]\n",
      " [-0.7944538 ]\n",
      " [-0.97052705]\n",
      " [-0.71252966]\n",
      " [ 0.95814681]\n",
      " [ 0.50040412]\n",
      " [ 0.41395468]\n",
      " [ 0.01961884]\n",
      " [-0.36207476]\n",
      " [ 0.0747766 ]\n",
      " [-0.97882289]\n",
      " [-0.79214132]\n",
      " [-0.59230739]\n",
      " [ 0.73408157]\n",
      " [ 0.75951695]\n",
      " [-0.54085475]\n",
      " [-0.03548805]\n",
      " [ 0.70868784]\n",
      " [ 1.01554716]\n",
      " [ 0.3598974 ]\n",
      " [ 0.18038218]\n",
      " [ 0.70964849]\n",
      " [-0.60463977]\n",
      " [ 0.16329235]\n",
      " [-0.83828813]\n",
      " [-0.71010029]\n",
      " [-0.52556252]\n",
      " [ 0.6258198 ]\n",
      " [ 0.40616217]\n",
      " [-0.6864723 ]\n",
      " [ 0.35487333]]\n",
      "[ 0.6648221  -0.13442098  0.12021134  0.29980177  0.66667026  0.99242455\n",
      " -0.83569193 -0.43488869  0.43297255  0.84911418  0.89404953  0.92100531\n",
      "  0.18640119 -0.97163701  0.09457441 -0.6987316  -0.96356362  0.97461015\n",
      "  0.83057958 -1.03515971  0.18870476 -0.24388491 -0.15770234 -0.52269059\n",
      "  0.44836867 -0.65024751  0.40866226  0.59853047  0.84041125  0.36931834\n",
      " -1.06323826 -0.73695004  0.62056899  0.25382885 -0.88106561 -0.79485029\n",
      "  1.00037944  0.76806515 -0.46049687 -0.2815057  -1.03491771 -0.04933892\n",
      " -0.81472135 -0.90826076 -0.04304821  0.11996999 -0.52147824  0.6850695\n",
      " -0.64645034  0.97662383  0.79489553 -0.99774849 -0.29428026  0.0637196\n",
      "  0.26060548  0.24212538  0.40054131  0.04762298 -0.55154943  0.54900604\n",
      " -1.01702237 -0.62226433 -0.28317901 -0.49782991]\n",
      "[[-0.11219794 -0.10563568 -0.00470929 ...,  0.08590627  0.0682646\n",
      "   0.11699007]\n",
      " [-0.07270392 -0.01558956 -0.07439937 ..., -0.06367459 -0.04498153\n",
      "  -0.0747614 ]\n",
      " [-0.14240441  0.0500006  -0.03887844 ...,  0.01367463  0.05421239\n",
      "   0.02556208]\n",
      " ..., \n",
      " [-0.09262732 -0.12830997  0.13815193 ...,  0.0559182   0.02104764\n",
      "  -0.09813505]\n",
      " [ 0.1165801   0.00282336 -0.06808689 ...,  0.04944118  0.11475753\n",
      "   0.00816371]\n",
      " [-0.12726076  0.04423345 -0.05754    ...,  0.07925954 -0.11378912\n",
      "  -0.09254042]]\n",
      "[-0.06802198  0.12323578  0.08863645  0.22273274 -0.08577448  0.0337978\n",
      " -0.16743562 -0.04326596  0.10780944  0.05937283 -0.11737066 -0.09234153\n",
      " -0.19657649  0.1066627  -0.12406391 -0.13239437 -0.04845738  0.11514157\n",
      "  0.09829336  0.10203557  0.03692336  0.11818876 -0.08232633 -0.12450118\n",
      " -0.07784461  0.12723677  0.12165393 -0.06346768 -0.18544568 -0.12904102\n",
      "  0.02213129  0.08500236  0.077086    0.0766831  -0.01942164  0.12052846\n",
      " -0.18078008 -0.14310338 -0.00579624  0.1186725  -0.03029304  0.10771948\n",
      " -0.16359086 -0.01074633 -0.01846598  0.13552155 -0.06444149  0.00918549\n",
      " -0.12513019  0.11854029 -0.15457922 -0.05718606 -0.0794119  -0.09217029\n",
      "  0.1341313  -0.11631164 -0.10174467  0.06021003 -0.08993602 -0.10849496\n",
      " -0.07991319 -0.06983025  0.11130699 -0.0041032 ]\n",
      "[[ 0.02249908 -0.0123829   0.01674061  0.01122223  0.01242664  0.01051508\n",
      "   0.01141385 -0.01704473  0.03997204  0.01627107 -0.02148911 -0.00385506\n",
      "   0.01152908  0.02393294  0.01359286  0.03626594 -0.00681993  0.03376576\n",
      "  -0.01589769  0.00466481  0.00160887  0.00080033  0.00382784  0.00829977\n",
      "   0.01445376 -0.01298018 -0.01297851  0.02530994  0.02621642  0.0155763\n",
      "   0.01767451  0.00767558  0.02749909 -0.00975846  0.00807593 -0.01413715\n",
      "  -0.00909271 -0.01675765 -0.0245373  -0.02026066  0.01579659 -0.00105703\n",
      "   0.02139737 -0.00817903 -0.0063396  -0.00680235 -0.01125748 -0.00261427\n",
      "  -0.03010522 -0.0170422  -0.00762075  0.00444522  0.00668132 -0.0144953\n",
      "  -0.03772894 -0.01987967 -0.01590867  0.01628761  0.00142395 -0.00096723\n",
      "   0.00436146  0.01944718 -0.00445155 -0.01988054]]\n",
      "[-0.0317843]\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
